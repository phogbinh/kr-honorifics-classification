{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf80011",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5420725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"klue/bert-base\"\n",
    "MODEL_LEARNING_RATE = 1e-5\n",
    "MODEL_BATCH_SIZE = 128\n",
    "MODEL_EPOCHS_NUM = 10\n",
    "MODEL_OUTPUT_DIR = \"trained_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13cc5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f7e00",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834d9666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d039a187e0a04ce4\n",
      "Found cached dataset csv (/home/bill/.cache/huggingface/datasets/csv/default-d039a187e0a04ce4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11e0770438345c383996fb9cb22794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'honorific'],\n",
       "        num_rows: 24000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['sentence', 'honorific'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'honorific'],\n",
       "        num_rows: 4800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"eval\": \"eval.csv\", \"test\": \"test.csv\"})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b752713f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>honorific</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>내일 반품할 노트북이 삼성인지 엘지인지 알려주세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내일 삼성노트북을 반품할거에요 엘지노트북을 반품할거에요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>내일 삼성노트북이랑 엘지노트북 중에 어떤 노트북을 반품하는거에요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>혹시 내일 반품할 노트북이 삼성과 엘지 중에 무엇인지 아시나요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>내일 노트북 반품이 삼성노트북인지 엘지노트북인지 확인부탁드립니다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sentence  honorific\n",
       "0          내일 반품할 노트북이 삼성인지 엘지인지 알려주세요          1\n",
       "1       내일 삼성노트북을 반품할거에요 엘지노트북을 반품할거에요          1\n",
       "2  내일 삼성노트북이랑 엘지노트북 중에 어떤 노트북을 반품하는거에요          1\n",
       "3   혹시 내일 반품할 노트북이 삼성과 엘지 중에 무엇인지 아시나요          1\n",
       "4  내일 노트북 반품이 삼성노트북인지 엘지노트북인지 확인부탁드립니다          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f5a27",
   "metadata": {},
   "source": [
    "# Set tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb00f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5420, 3840, 2406, 2265, 2547, 2052, 2379, 17360, 2406, 2265, 2547, 1570, 2170, 3711, 11161, 2069, 24183, 2205, 2259, 2180, 2170, 2182, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer(\"내일 삼성노트북이랑 엘지노트북 중에 어떤 노트북을 반품하는거에요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbd464",
   "metadata": {},
   "source": [
    "# Set encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20dd4f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>honorific</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   honorific\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"honorific\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fbc74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder().fit(df[[\"honorific\"]])\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c966db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(pd.DataFrame({\"honorific\": [1]})).toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37980c55",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40c65b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bill/.cache/huggingface/datasets/csv/default-d039a187e0a04ce4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-88a7f6e85c0b3612.arrow\n",
      "Loading cached processed dataset at /home/bill/.cache/huggingface/datasets/csv/default-d039a187e0a04ce4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-92441cd9a8defedd.arrow\n",
      "Loading cached processed dataset at /home/bill/.cache/huggingface/datasets/csv/default-d039a187e0a04ce4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7c5c86aff84acbef.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'honorific', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 24000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['sentence', 'honorific', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'honorific', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 4800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(dataslice):\n",
    "  tokenized_inputs = tokenizer(dataslice[\"sentence\"])\n",
    "  labels = []\n",
    "  for honorific in dataslice[\"honorific\"]:\n",
    "    encoded_honorific = encoder.transform(pd.DataFrame({\"honorific\": [honorific]})).toarray()[0]\n",
    "    labels.append(encoded_honorific)\n",
    "  tokenized_inputs[\"label\"] = labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "processed_dataset = dataset.map(process, batched=True)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1fe367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '내일 삼성노트북을 반품할거에요 엘지노트북을 반품할거에요',\n",
       " 'honorific': 1,\n",
       " 'input_ids': [2,\n",
       "  5420,\n",
       "  3840,\n",
       "  2406,\n",
       "  2265,\n",
       "  2547,\n",
       "  2069,\n",
       "  24183,\n",
       "  2085,\n",
       "  2180,\n",
       "  2170,\n",
       "  2182,\n",
       "  17360,\n",
       "  2406,\n",
       "  2265,\n",
       "  2547,\n",
       "  2069,\n",
       "  24183,\n",
       "  2085,\n",
       "  2180,\n",
       "  2170,\n",
       "  2182,\n",
       "  3],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'label': [0.0, 1.0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540211b",
   "metadata": {},
   "source": [
    "# Set categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040376b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = encoder.categories_[0]\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6a927",
   "metadata": {},
   "source": [
    "# Set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dffd724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79522b7c",
   "metadata": {},
   "source": [
    "# Set trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73e4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=MODEL_OUTPUT_DIR,\n",
    "  learning_rate=MODEL_LEARNING_RATE,\n",
    "  per_device_train_batch_size=MODEL_BATCH_SIZE,\n",
    "  per_device_eval_batch_size=MODEL_BATCH_SIZE,\n",
    "  num_train_epochs=MODEL_EPOCHS_NUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91cfc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=processed_dataset[\"train\"],\n",
    "  eval_dataset=processed_dataset[\"eval\"],\n",
    "  tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334ae3a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64c2c045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, honorific. If sentence, honorific are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bill/miniconda3/envs/krenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 24000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1880\n",
      "  Number of trainable parameters = 110618882\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1880' max='1880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1880/1880 09:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_model/checkpoint-500\n",
      "Configuration saved in trained_model/checkpoint-500/config.json\n",
      "Model weights saved in trained_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to trained_model/checkpoint-1000\n",
      "Configuration saved in trained_model/checkpoint-1000/config.json\n",
      "Model weights saved in trained_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to trained_model/checkpoint-1500\n",
      "Configuration saved in trained_model/checkpoint-1500/config.json\n",
      "Model weights saved in trained_model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in trained_model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in trained_model/checkpoint-1500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "if train_model:\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40ad0e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/config.json\n",
      "Model weights saved in trained_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "  model.save_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf5b62",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82f6eb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file trained_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trained_model = BertForSequenceClassification.from_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ad55514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['삼성과 엘지 중에 에어컨이 더 저렴한 곳 확인 부탁드립니다',\n",
       " '에어컨이 더 저렴한 곳은 어디인가요 삼성과 엘지 중에',\n",
       " '삼성 에어컨이 더 저렴한가요 엘지 에어컨이 더 저렴한가요',\n",
       " '에어컨이 더 저렴한 곳은 삼성인가요 엘지인가요',\n",
       " '삼성과 엘지 중에 에어컨이 더 저렴한 곳을 아십니까']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = processed_dataset[\"test\"][\"sentence\"]\n",
    "test_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3af9a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.4885,  7.5042],\n",
       "        [-7.4665,  7.4227],\n",
       "        [-7.4214,  7.5274],\n",
       "        [-7.4219,  7.4593],\n",
       "        [-7.4195,  7.3443]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer(test_texts, padding=True, return_tensors=\"pt\")\n",
    "logits = trained_model(**model_inputs).logits\n",
    "logits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "779e46fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0816e-07, 1.0000e+00],\n",
       "        [3.4176e-07, 1.0000e+00],\n",
       "        [3.2196e-07, 1.0000e+00],\n",
       "        [3.4450e-07, 1.0000e+00],\n",
       "        [3.8744e-07, 1.0000e+00]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "predicted_probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "predicted_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d81e4fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_labels = [categories[label_idx]\n",
    "                    for label_idx in np.argmax(predicted_probabilities.detach().numpy(), axis=1)]\n",
    "predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3436a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 삼성과 엘지 중에 에어컨이 더 저렴한 곳 확인 부탁드립니다\n",
      "1: 에어컨이 더 저렴한 곳은 어디인가요 삼성과 엘지 중에\n",
      "1: 삼성 에어컨이 더 저렴한가요 엘지 에어컨이 더 저렴한가요\n",
      "1: 에어컨이 더 저렴한 곳은 삼성인가요 엘지인가요\n",
      "1: 삼성과 엘지 중에 에어컨이 더 저렴한 곳을 아십니까\n",
      "1: 넷플릭스랑 왓챠 중에 가성비가 더 좋은 곳을 알려주세요\n",
      "1: 가성비가 더 좋은 곳이 넷플릭스인지 왓챠인지 확인부탁드립니다\n",
      "1: 넷플릭스가 가성비가 더 좋아요 왓챠가 가성비가 더 좋아요\n",
      "1: 넷플릭스와 왓챠 중에 어디가 더 가성비가 좋은지 좀 알려주세요\n",
      "1: 가성비가 더 좋은 곳 확인해주세요 넷플릭스랑 왓챠 중에\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "  print(f\"{predicted_labels[idx]}: {test_texts[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d4ebec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9977083333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects_num = 0\n",
    "for idx in range(len(predicted_labels)):\n",
    "  if predicted_labels[idx] == processed_dataset[\"test\"][\"honorific\"][idx]:\n",
    "    corrects_num += 1\n",
    "corrects_num / len(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
